{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za8U9qG7K39J",
        "outputId": "9983cc27-4129-40b1-809b-47589c8e5074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added reward5.Toatal now =5\n",
            "added reward-2.Toatal now =3\n"
          ]
        }
      ],
      "source": [
        "class RewardTracker:\n",
        "  def __init__(self):\n",
        "    self.total_reward=0\n",
        "  def add_reward(self,reward):\n",
        "    self.total_reward+=reward\n",
        "    print(f\"added reward{reward}.Toatal now ={self.total_reward}\")\n",
        "#usage\n",
        "tracker=RewardTracker()\n",
        "tracker.add_reward(5)\n",
        "tracker.add_reward(-2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Agent:\n",
        "  def __init__(self,actions):\n",
        "    self.actions=actions\n",
        "  def choose_action(self):\n",
        "    action=random.choice(self.actions)\n",
        "    print(f\"Agent chose action:{action}\")\n",
        "    return action\n",
        "#usage\n",
        "agent=Agent([\"left\",\"right\",\"up\",\"down\"])\n",
        "agent.choose_action()\n",
        "agent.choose_action()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5KYB4XgYNuXF",
        "outputId": "1bd30984-2197-4dc1-dd8f-92a62ff3aa22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent chose action:left\n",
            "Agent chose action:left\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'left'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "  def __init__(self):\n",
        "    self.state=0\n",
        "  def step(self,action):\n",
        "    if action ==\"right\":\n",
        "     self.state+=1\n",
        "    elif ction ==\"left\":\n",
        "      self.state -=1\n",
        "    reward=1 if self.state ==2 else 0\n",
        "    print(f\"state: {self.state}, Reward : {reward }\")\n",
        "    return self.state,reward\n",
        "env=Environment()\n",
        "env.step(\"right\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk8Z9FvPPZL7",
        "outputId": "8aa62938-6522-46c9-9fd4-f5a3c422ea6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: 1, Reward : 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "class MultiArmedBandit:\n",
        "  def __init__(self,k=10):\n",
        "    self.k=k\n",
        "    self.true_rewards=np.random.normal(0,1,k)\n",
        "    self.estimates=np.zeros(k)\n",
        "    self.action_counts=np.zeros(k)\n",
        "    self.total_steps=0\n",
        "  def pull_arm(self,action):\n",
        "    reward=np.random.noramal(self.true_reward[action],1)\n",
        "    return reward\n",
        "  def update_estimates(self,action,reward):\n",
        "    self.action_counts[action]+=1\n",
        "    self.estimates[action]+=(reward-self.estimates[action]/self.action_count)\n",
        "  def select_action(self,epsilon=0.1):\n",
        "    if np.random.rand()<epsilon:\n",
        "      return np.random.randit(self.k)\n",
        "    else:\n",
        "      return np.argmax(self.estimates)\n",
        "#k=arms simulation\n",
        "def run_bandit(k=10,steps=1000,epsilon=0.1):\n",
        "  bandit=MultiArmedBandit(k)\n",
        "  rewards=[]\n",
        "  for step in range(steps):\n",
        "    action=bandit.select_action(epsilon)\n",
        "    reward=bandit.pull_arm(action)\n",
        "    bandit.update_estimates(action,reward)\n",
        "    rewards.append(reward)\n",
        "  return rewards,bandit\n",
        "#run experiment\n",
        "rewards,bandit=run_bandit(k=10,steps=1000,epsilon=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "01YQ98JS8NB9",
        "outputId": "aab2e1dc-d970-4ddc-fdb9-0b029ce92266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy.random' has no attribute 'noramal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4094476554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_bandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4094476554.py\u001b[0m in \u001b[0;36mrun_bandit\u001b[0;34m(k, steps, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_estimates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4094476554.py\u001b[0m in \u001b[0;36mpull_arm\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpull_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoramal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_estimates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.random' has no attribute 'noramal'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#import lib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "diabetes =load_diabetes()\n",
        "df=pd.DataFrame(diabetes.data,columns=diabetes.feature_names)\n",
        "df['target']=diabetes.target #add target column\n",
        "print(\"shape of dataset :\",df.shape)\n",
        "print(df.head(10))\n",
        "x=diabetes.data #features\n",
        "y=diabetes.target #target\n",
        "print(\"Dataset shape:\",x.shape)\n",
        "print(\"target shape \",y.shape)\n",
        "model=LinearRegression()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Testing data shape:\", x_test.shape)\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "mse=mean_squared_error(y_test,y_pred)\n",
        "r2=r2_score(y_test,y_pred)\n",
        "print(\"Mean squared error:\",mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPdzmTYePdTz",
        "outputId": "6eee4cde-2fe9-4713-d9bc-38f5ac134407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of dataset : (442, 11)\n",
            "        age       sex       bmi        bp        s1        s2        s3  \\\n",
            "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
            "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
            "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
            "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
            "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
            "5 -0.092695 -0.044642 -0.040696 -0.019442 -0.068991 -0.079288  0.041277   \n",
            "6 -0.045472  0.050680 -0.047163 -0.015999 -0.040096 -0.024800  0.000779   \n",
            "7  0.063504  0.050680 -0.001895  0.066629  0.090620  0.108914  0.022869   \n",
            "8  0.041708  0.050680  0.061696 -0.040099 -0.013953  0.006202 -0.028674   \n",
            "9 -0.070900 -0.044642  0.039062 -0.033213 -0.012577 -0.034508 -0.024993   \n",
            "\n",
            "         s4        s5        s6  target  \n",
            "0 -0.002592  0.019907 -0.017646   151.0  \n",
            "1 -0.039493 -0.068332 -0.092204    75.0  \n",
            "2 -0.002592  0.002861 -0.025930   141.0  \n",
            "3  0.034309  0.022688 -0.009362   206.0  \n",
            "4 -0.002592 -0.031988 -0.046641   135.0  \n",
            "5 -0.076395 -0.041176 -0.096346    97.0  \n",
            "6 -0.039493 -0.062917 -0.038357   138.0  \n",
            "7  0.017703 -0.035816  0.003064    63.0  \n",
            "8 -0.002592 -0.014960  0.011349   110.0  \n",
            "9 -0.002592  0.067737 -0.013504   310.0  \n",
            "Dataset shape: (442, 10)\n",
            "target shape  (442,)\n",
            "Training data shape: (353, 10)\n",
            "Testing data shape: (89, 10)\n",
            "Mean squared error: 2900.193628493482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow -q\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.datasets simport Dense\n",
        "from"
      ],
      "metadata": {
        "id": "qPxmJbTKVXaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f0a0ca95-d82f-49cd-a497-2af7d5b2771e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-4171639281.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4171639281.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    from sklearn.datasets simport Dense\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from robot import RecyclingRobot\n",
        "# Initialize the RecyclingRobot instance with parameters\n",
        "\n",
        "robot = RecyclingRobot(\n",
        "    alpha=0.1,      # Learning rate: Determines how quickly the Q-values are updated.\n",
        "    gamma=0.9,      # Discount factor: Weighs the importance of future rewards compared to immediate rewards.\n",
        "    epsilon=0.1,    # Exploration rate: Probability of taking a random action instead of the best-known action (used for exploration in the epsilon-greedy policy).\n",
        "    episodes=10000, # Number of training episodes: Defines how many iterations the robot will undergo to learn.\n",
        "    seed=42,        # Random seed: Ensures reproducibility of the results by fixing the random number generation.\n",
        "    prob_alpha=0.9, # Transition probability α: Likelihood of staying in a high-energy state after searching.\n",
        "    prob_beta=0.5,  # Transition probability β: Likelihood of staying in a low-energy state after searching.\n",
        "    r_search=2,     # Reward for searching: Expected reward from searching (e.g., number of cans found).\n",
        "    r_wait=0.5      # Reward for waiting: Expected reward while waiting (often smaller than r_search).\n",
        ")\n",
        "# Train the robot\n",
        "print(\"Training the Recycling Robot...\")\n",
        "robot.train()\n",
        "print(\"Training completed.\")\n",
        "# Display the Q-table\n",
        "robot.display_q_table()\n",
        "# Plot the Q-values\n",
        "print(\"Plotting the Q-values...\")\n",
        "robot.plot_q_values()"
      ],
      "metadata": {
        "id": "Bgt3o9PVddZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "43418cca-7c34-422f-f1a0-53257230b07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'robot'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3270884964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrobot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecyclingRobot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Initialize the RecyclingRobot instance with parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m robot = RecyclingRobot(\n\u001b[1;32m      5\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# Learning rate: Determines how quickly the Q-values are updated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'robot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile robot.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "class RecyclingRobot:\n",
        "    def __init__(self, alpha, gamma, epsilon, episodes, seed, prob_alpha, prob_beta, r_search, r_wait):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.episodes = episodes\n",
        "        self.prob_alpha = prob_alpha\n",
        "        self.prob_beta = prob_beta\n",
        "        self.r_search = r_search\n",
        "        self.r_wait = r_wait\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # States and actions\n",
        "        self.states = [\"high\", \"low\"]\n",
        "        self.actions = [\"search\", \"wait\"]\n",
        "        self.q_table = {state: {action: 0.0 for action in self.actions} for state in self.states}\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(self.actions)\n",
        "        return max(self.q_table[state], key=self.q_table[state].get)\n",
        "\n",
        "    def get_next_state_reward(self, state, action):\n",
        "        if state == \"high\":\n",
        "            if action == \"search\":\n",
        "                next_state = \"high\" if random.random() < self.prob_alpha else \"low\"\n",
        "                reward = self.r_search\n",
        "            else:\n",
        "                next_state, reward = \"high\", self.r_wait\n",
        "        else:  # low energy\n",
        "            if action == \"search\":\n",
        "                next_state, reward = \"high\", 0\n",
        "            else:\n",
        "                next_state = \"low\" if random.random() < self.prob_beta else \"high\"\n",
        "                reward = self.r_wait\n",
        "        return next_state, reward\n",
        "\n",
        "    def train(self):\n",
        "        for _ in range(self.episodes):\n",
        "            state = random.choice(self.states)\n",
        "            while True:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward = self.get_next_state_reward(state, action)\n",
        "                best_next = max(self.q_table[next_state].values())\n",
        "                self.q_table[state][action] += self.alpha * (reward + self.gamma * best_next - self.q_table[state][action])\n",
        "                if random.random() < 0.1:  # small chance to end episode\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "    def display_q_table(self):\n",
        "        print(\"Q-Table:\")\n",
        "        for s in self.q_table:\n",
        "            print(f\"{s}: {self.q_table[s]}\")\n",
        "\n",
        "    def plot_q_values(self):\n",
        "        plt.figure(figsize=(6,4))\n",
        "        for action in self.actions:\n",
        "            plt.bar([s + \"-\" + action for s in self.states],\n",
        "                    [self.q_table[s][action] for s in self.states])\n",
        "        plt.title(\"Q-values after training\")\n",
        "        plt.ylabel(\"Q-value\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M33uetvKa0N0",
        "outputId": "e53b58f4-fa58-4d60-d015-70727d614596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting robot.py\n"
          ]
        }
      ]
    }
  ]
}